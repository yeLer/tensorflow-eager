{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Execution 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 的 Eager Execution 是一种命令式编程环境，可立即评估操作，无需构建图：操作会返回具体的值，而不是构建以后再运行的计算图。Eager 模式使得 TensorFlow 的使用和调试变得简单。为了真实地感受 Eager 模式的魅力，推荐大家在交互式 python 解释器中运行本文的代码。\n",
    "\n",
    "Eager Execution 是一个灵活的机器学习平台，用于研究和实验，可提供：\n",
    "\n",
    "    更好的交互 ---------- 可以更自然地组织你的代码，可以直接使用 Python 数据结构。\n",
    "    更容易的调试 ---------- 可以直接调用 op 去检查模型的运行过程，测试更改后的模型。 使用标准的 Python 调试工具来检测模型的错误。\n",
    "    自然的控制流 ---------- 可以使用 Python 控制流，从而简化动态模型的搭建。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager 模式支持绝大多数 TensorFlow op 和 GPU 加速。Eager 模式的使用示例，详见tensorflow/contrib/eager/python/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Eager 模式的开启、基本用法 ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要开启 Eager 模式，只需要在 “程序” 或 “控制台会话” 的开始添加 tf.enable_eager_execution()。不要 将开启 Eager 模式的语句放在程序的其它地方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))  # => \"hello, [[4.]]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启用 Eager Execution 会改变 TensorFlow 操作的行为方式 — 现在它们会立即评估并将值返回给 Python。tf.Tensor 对象会引用具体值，而不是指向计算图中的节点的符号句柄。由于 Eager 模式不需要构建稍后在会话中运行的计算图，因此使用 print() 或调试程序很容易检查结果。评估、输出和检查张量值不会中断计算梯度的流程。\n",
    "\n",
    "Eager Execution 适合与 NumPy 一起使用。NumPy op 可以以 tf.Tensor 为参数。TensorFlow math operations 将 Python 对象和 NumPy 数组转换为 tf.Tensor 对象。tf.Tensor.numpy 方法返回对象的值作为 NumPy ndarray。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2],\n",
    "                 [3,4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting support\n",
    "b = tf.add(a,1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Operator overloading is supported\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6]\n",
      " [12 20]]\n"
     ]
    }
   ],
   "source": [
    "# Use NumPy values\n",
    "import numpy as np\n",
    "c = np.multiply(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Obtain numpy value from a tensor\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.contrib.eager 模块的 op 可以同时适用于 eager 和 graph 环境。这在编写兼容 graph 模式的的代码时非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Eager 模式下 model 的建立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多机器学习模型通常由很多层组合而成。开启 eager 模式后，你可以自定义层 或 使用 tf.keras.layers 里的层。\n",
    "\n",
    "在使用 Python 对象来表示一个层时，需要从 tf.keras.layers.Layer 继承来编写自定义层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will be decraped\n",
    "class MySimpleLayer(tf.keras.layers.Layer): \n",
    "    def __init__(self, output_units): \n",
    "        super(MySimpleLayer, self).__init__() \n",
    "        self.output_units = output_units \n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        # The build method gets called the first time your layer is used. \n",
    "        # Creating variables on build() allows you to make their shape depend \n",
    "        # on the input shape and hence removes the need for the user to specify \n",
    "        # full shapes. It is possible to create variables during __init__() if \n",
    "        # you already know their full shapes. \n",
    "        self.kernel = self.add_variable( \"kernel\", [input_shape[-1], self.output_units]) \n",
    "    \n",
    "    def call(self, input): # Override call() instead of __call__ so we can perform some bookkeeping. \n",
    "        return tf.matmul(input, self.kernel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 tf.keras.layers.Dense 来取代 MySimpleLayer 非常有必要（因为 Dense 的功能包含了 MySimpleLayer 的功能）。\n",
    "\n",
    "将层组合成模型时，你可以使用 tf.keras.Sequential 来表示模型，其可以线性地多个层堆叠起来。Sequential 在搭建简单模型时非常有用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了使用 Sequential 来构建模型，也可以从 tf.keras.Model 继承来编写一个模型类表示模型。构建的模型包含了很多层，允许 tf.keras.Model 包含其它 tf.keras.Model 对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MNISTModel, self).__init__()\n",
    "    self.dense1 = tf.keras.layers.Dense(units=10)\n",
    "    self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "  def call(self, input):\n",
    "    \"\"\"Run the model.\"\"\"\n",
    "    result = self.dense1(input)\n",
    "    result = self.dense2(result)\n",
    "    result = self.dense2(result)  # reuse variables from dense2 layer\n",
    "    return result\n",
    "\n",
    "model = MNISTModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 tf.keras.Model 类中不需要设置输入的 shape。\n",
    "\n",
    "tf.keras.layers 类 创建、包含了该层的模型参数，这些参数 与 实例化出的对象 共存亡。当需要共享层的参数时，共享 实例化出的对象 即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Eager 模式下 model 的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 计算梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动微分对于机器学习算法的训练至关重要。在 Eager 模式，为了计算梯度，需要使用 tf.GradientTape 来追踪 op。\n",
    "\n",
    "tf.GradientTape 是一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 训练一个模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使不进行训练，也可以在 eager 模式调用模型并检查模型的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor representing a blank image\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "print(batch.shape)  # => (1, 1, 784)\n",
    "\n",
    "result = model(batch)\n",
    "# => tf.Tensor([[[ 0.  0., ..., 0.]]], shape=(1, 1, 10), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的例子使用 TensorFlow MNIST example 中的 dataset.py module 来导入数据，请将该模块下载到你的机器。运行下面的代码去下载 MNIST 数据集到你的机器，并以 tf.data.Dataset 的形式将数据集用于训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/tmpymqfd_nc.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /tmp/tmpu8li5gx_.gz\n"
     ]
    }
   ],
   "source": [
    "import dataset  # download dataset.py file\n",
    "dataset_train = dataset.train('./datasets').shuffle(60000).repeat(4).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了训练一个模型，需要去定义一个损失函数、计算梯度。使用优化器去更新模型变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 2.319\n",
      "Loss at step 0000: 2.304\n",
      "Loss at step 0200: 1.879\n",
      "Loss at step 0400: 1.802\n",
      "Loss at step 0600: 1.546\n",
      "Loss at step 0800: 1.526\n",
      "Loss at step 1000: 1.428\n",
      "Loss at step 1200: 1.314\n",
      "Loss at step 1400: 1.282\n",
      "Loss at step 1600: 1.040\n",
      "Loss at step 1800: 1.022\n",
      "Loss at step 2000: 0.900\n",
      "Loss at step 2200: 1.023\n",
      "Loss at step 2400: 0.777\n",
      "Loss at step 2600: 0.807\n",
      "Loss at step 2800: 0.808\n",
      "Loss at step 3000: 0.843\n",
      "Loss at step 3200: 0.739\n",
      "Loss at step 3400: 0.972\n",
      "Loss at step 3600: 0.557\n",
      "Loss at step 3800: 0.760\n",
      "Loss at step 4000: 0.452\n",
      "Loss at step 4200: 0.678\n",
      "Loss at step 4400: 0.662\n",
      "Loss at step 4600: 0.428\n",
      "Loss at step 4800: 0.745\n",
      "Loss at step 5000: 0.545\n",
      "Loss at step 5200: 0.439\n",
      "Loss at step 5400: 0.457\n",
      "Loss at step 5600: 0.439\n",
      "Loss at step 5800: 0.440\n",
      "Loss at step 6000: 0.595\n",
      "Loss at step 6200: 0.734\n",
      "Loss at step 6400: 0.887\n",
      "Loss at step 6600: 0.721\n",
      "Loss at step 6800: 0.565\n",
      "Loss at step 7000: 0.418\n",
      "Loss at step 7200: 0.699\n",
      "Loss at step 7400: 0.502\n",
      "Final loss: 0.477\n"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y):\n",
    "  prediction = model(x)\n",
    "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "x, y = iter(dataset_train).next()\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n",
    "\n",
    "# Training loop\n",
    "for (i, (x, y)) in enumerate(dataset_train):\n",
    "  # Calculate derivatives of the input function with respect to its parameters.\n",
    "  grads = grad(model, x, y)\n",
    "  # Apply the gradient to the model\n",
    "  optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 200 == 0:\n",
    "    print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "并且为了更快地训练，将上面的计算使用 GPU 进行加速："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0000: 0.247\n",
      "Loss at step 0200: 0.362\n",
      "Loss at step 0400: 0.299\n",
      "Loss at step 0600: 0.194\n",
      "Loss at step 0800: 0.645\n",
      "Loss at step 1000: 0.400\n",
      "Loss at step 1200: 0.357\n",
      "Loss at step 1400: 0.233\n",
      "Loss at step 1600: 0.288\n",
      "Loss at step 1800: 0.346\n",
      "Loss at step 2000: 0.360\n",
      "Loss at step 2200: 0.215\n",
      "Loss at step 2400: 0.423\n",
      "Loss at step 2600: 0.354\n",
      "Loss at step 2800: 0.318\n",
      "Loss at step 3000: 0.215\n",
      "Loss at step 3200: 0.224\n",
      "Loss at step 3400: 0.378\n",
      "Loss at step 3600: 0.374\n",
      "Loss at step 3800: 0.160\n",
      "Loss at step 4000: 0.305\n",
      "Loss at step 4200: 0.312\n",
      "Loss at step 4400: 0.208\n",
      "Loss at step 4600: 0.351\n",
      "Loss at step 4800: 0.247\n",
      "Loss at step 5000: 0.274\n",
      "Loss at step 5200: 0.305\n",
      "Loss at step 5400: 0.419\n",
      "Loss at step 5600: 0.393\n",
      "Loss at step 5800: 0.343\n",
      "Loss at step 6000: 0.608\n",
      "Loss at step 6200: 0.234\n",
      "Loss at step 6400: 0.756\n",
      "Loss at step 6600: 0.101\n",
      "Loss at step 6800: 0.150\n",
      "Loss at step 7000: 0.428\n",
      "Loss at step 7200: 0.352\n",
      "Loss at step 7400: 0.250\n",
      "Final loss: 0.234\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "  for (i, (x, y)) in enumerate(dataset_train):\n",
    "    # minimize() is equivalent to the grad() and apply_gradients() calls.\n",
    "    optimizer.minimize(lambda: loss(model, x, y),\n",
    "                       global_step=tf.train.get_or_create_global_step())\n",
    "    if i % 200 == 0:\n",
    "      print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 变量和优化器 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfe.Variable 对象存储的 tf.Tensor 的值可变，这使得自动微分过程变得简单。模型的变量可以被封装到类中。\n",
    "\n",
    "通过使用 tfe.Variable 及 tf.GradientTape 可以更好地封装模型参数。例如，上面的自动微分例子可以重写为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.364\n",
      "Loss at step 000: 66.646\n",
      "Loss at step 020: 30.152\n",
      "Loss at step 040: 13.932\n",
      "Loss at step 060: 6.721\n",
      "Loss at step 080: 3.516\n",
      "Loss at step 100: 2.091\n",
      "Loss at step 120: 1.458\n",
      "Loss at step 140: 1.176\n",
      "Loss at step 160: 1.051\n",
      "Loss at step 180: 0.995\n",
      "Loss at step 200: 0.970\n",
      "Loss at step 220: 0.959\n",
      "Loss at step 240: 0.954\n",
      "Loss at step 260: 0.952\n",
      "Loss at step 280: 0.951\n",
      "Final loss: 0.951\n",
      "W = 2.9642889499664307, B = 2.0164880752563477\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.W = tfe.Variable(5., name='weight')\n",
    "    self.B = tfe.Variable(10., name='bias')\n",
    "  def call(self, inputs):\n",
    "    return inputs * self.W + self.B\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "  error = model(inputs) - targets\n",
    "  return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "# Define:\n",
    "# 1. A model.\n",
    "# 2. Derivatives of a loss function with respect to model parameters.\n",
    "# 3. A strategy for updating the variables based on the derivatives.\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# Training loop\n",
    "for i in range(300):\n",
    "  grads = grad(model, training_inputs, training_outputs)\n",
    "  optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Eager 模式下，模型状态（变量的状态）及存活时间由与之对应的 Python 对象决定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 graph 模式时，程序的状态（例如变量）储存在 “全局容器” 中，并且它们的存活时间由 tf.Session 对象来管理。与 graph 模式不同，在 eager 模式，程序的状态由其对应的 Python 对象决定，并且状态的存活时间由对应 Python 对象决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Variables 是对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Eager 模式下，变量的存活时间 由 与之对应的Python对象的存活时间决定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"gpu:0\"):\n",
    "  v = tfe.Variable(tf.random_normal([1000, 1000]))\n",
    "  v = None  # v no longer takes up GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 基于对象的保存（Object-based saving）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfe.Checkpoint 能够将 tfe.Variables 储存为 checkpoints，能够从 checkpoints 中恢复 tfe.Variable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n"
     ]
    }
   ],
   "source": [
    "x = tfe.Variable(10.)\n",
    "\n",
    "checkpoint = tfe.Checkpoint(x=x)  # save as \"x\"\n",
    "\n",
    "x.assign(2.)   # Assign a new value to the variables and save.\n",
    "save_path = checkpoint.save('./ckpt/'+\"exam\") # this will create a new folder named ckpt\n",
    "\n",
    "x.assign(11.)  # Change the variable after saving,but will not change the value.\n",
    "\n",
    "# Restore values from the checkpoint\n",
    "checkpoint.restore(save_path)\n",
    "\n",
    "print(x)  # => 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了保存和加载模型，tfe.Checkpoint 存储对象的内部状态，\n",
    "\n",
    "为了记录 [模型、优化器、global step]，只需要将它们传给一个 tfe.Checkpoint："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f7907057630>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "checkpoint_dir = './ckpt/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "root = tfe.Checkpoint(optimizer=optimizer,\n",
    "                      model=model,\n",
    "                      optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "root.save(file_prefix=checkpoint_prefix)\n",
    "# or\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 面对对象的指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tfe.metrics 被当做对象来存储。通过给指标传递新值来更新指标的值，取回指标的值使用 tfe.metrics.result 方法。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2061575, shape=(), dtype=float64, numpy=5.5>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tfe.metrics.Mean(\"loss\")\n",
    "m(0)\n",
    "m(5)\n",
    "m.result()  # => 2.5\n",
    "m([8, 9])\n",
    "m.result()  # => 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summaries and TensorBoard ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard 是一个用来理解、调试、优化模型训练过程的可视化工具。\n",
    "\n",
    "tf.contrib.summary 同时兼容 eager 和 graph 模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.contrib.summary.create_file_writer(logdir)\n",
    "global_step=tf.train.get_or_create_global_step()  # return global step var\n",
    "\n",
    "writer.set_as_default()\n",
    "\n",
    "for _ in range(iterations):\n",
    "  global_step.assign_add(1)\n",
    "  # Must include a record_summaries method\n",
    "  with tf.contrib.summary.record_summaries_every_n_global_steps(100): # record summary every 100 steps\n",
    "    # your model code goes here\n",
    "    tf.contrib.summary.scalar('loss', loss)\n",
    "     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5. Eager 自动微分的高级应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 动态模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.GradientTape 可以用于动态模型。下面的例子以类 NumPy 的代码实现了回溯搜索算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_search_step(fn, init_x, rate=1.0):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # Variables are automatically recorded, but manually watch a tensor\n",
    "    tape.watch(init_x)\n",
    "    value = fn(init_x)\n",
    "  grad = tape.gradient(value, init_x)\n",
    "  grad_norm = tf.reduce_sum(grad * grad)\n",
    "  init_value = value\n",
    "  while value > init_value - rate * grad_norm:\n",
    "    x = init_x - rate * grad\n",
    "    value = fn(x)\n",
    "    rate /= 2.0\n",
    "  return x, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 计算梯度的其它函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然 tf.GradientTape 是计算梯度的一个强有力 API，但有另一种 Autograd 风格的 API 来进行自动微分。如果运算的代码只与 tensor 及 梯度函数 相关，并且没有 tfe.Variables，这些函数是非常有用的。\n",
    "\n",
    "    tfe.gradients_function ---------- 返回一个函数，其计算输出相对于输入的梯度。\n",
    "    tfe.value_and_gradients_function ---------- 与 tfe.gradients_function 类似，但是当调用返回的函数时，其函数的输出值 和 梯度。\n",
    "\n",
    "在下面的例子中，tfe.gradients_function 以 square 函数为输入，返回一个计算 square 偏微分的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=2061621, shape=(), dtype=float32, numpy=-1.0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "  return tf.multiply(x, x)\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "\n",
    "square(3.)  # => 9.0\n",
    "grad(3.)    # => [6.0]\n",
    "\n",
    "# The second-order derivative of square:\n",
    "gradgrad = tfe.gradients_function(lambda x: grad(x)[0])\n",
    "gradgrad(3.)  # => [2.0]\n",
    "\n",
    "# The third-order derivative is None:\n",
    "gradgradgrad = tfe.gradients_function(lambda x: gradgrad(x)[0])\n",
    "gradgradgrad(3.)  # => [None]\n",
    "\n",
    "\n",
    "# With flow control:\n",
    "def abs(x):\n",
    "  return x if x > 0. else -x\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "\n",
    "grad(3.)   # => [1.0]\n",
    "grad(-3.)  # => [-1.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 自定义梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.custom_gradient 是一种覆盖梯度的简单方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def clip_gradient_by_norm(x, norm):\n",
    "  y = tf.identity(x)\n",
    "  def grad_fn(dresult):\n",
    "    return [tf.clip_by_norm(dresult, norm), None]\n",
    "  return y, grad_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.custom_gradient 通常被用来稳定梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=2061638, shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log1pexp(x):\n",
    "  return tf.log(1 + tf.exp(x))\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# The gradient computation works fine at x = 0.\n",
    "grad_log1pexp(0.)  # => [0.5]\n",
    "\n",
    "# However, x = 100 fails because of numerical instability.\n",
    "grad_log1pexp(100.)  # => [nan]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，log1pexp 函数可以被简化为一个custom gradient。下面的实现重用了 tf.exp(x) 的值，通过消除冗余的计算，使它变得更高效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=2061657, shape=(), dtype=float32, numpy=1.0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(dy):\n",
    "    return dy * (1 - 1 / (1 + e))\n",
    "  return tf.log(1 + e), grad\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# As before, the gradient computation works fine at x = 0.\n",
    "grad_log1pexp(0.)  # => [0.5]\n",
    "\n",
    "# And the gradient computation also works at x = 100.\n",
    "grad_log1pexp(100.)  # => [1.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Eager 模式的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 eager 模式下，模型的计算默认不使用 GPU。如果你想在 GPU 上运行模型，可以使用 tf.device('/gpu:0')。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to multiply a (1000, 1000) matrix by itself 200 times:\n",
      "CPU: 0.8703370094299316 secs\n",
      "GPU: 0.5488553047180176 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure(x, steps):\n",
    "  # TensorFlow initializes a GPU the first time it's used, exclude from timing.\n",
    "  tf.matmul(x, x)\n",
    "  start = time.time()\n",
    "  for i in range(steps):\n",
    "    x = tf.matmul(x, x)\n",
    "    _ = x.numpy()  # Make sure to execute op and not just enqueue it\n",
    "  end = time.time()\n",
    "  return end - start\n",
    "\n",
    "shape = (1000, 1000)\n",
    "steps = 200\n",
    "print(\"Time to multiply a {} matrix by itself {} times:\".format(shape, steps))\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "  print(\"CPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
    "\n",
    "# Run on GPU, if available:\n",
    "if tfe.num_gpus() > 0:\n",
    "  with tf.device(\"/gpu:0\"):\n",
    "    print(\"GPU: {} secs\".format(measure(tf.random_normal(shape), steps)))\n",
    "else:\n",
    "  print(\"GPU: not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于计算量较大的模型（比如 ResNet-50）在一个 GPU 上的计算，eager 模式的性能和 graph 是相当的。但是当 “模型的计算量比较小” 时，两个模式之间的性能差异就会变大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Eager 与 Graph 的兼容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager 使得开发、调试的交互性更好，而 graph 在分布式训练、性能优化、生产部署 方面有优势。但是编写 graph 代码比编写普通的 Python 代码难，并且不容易调试。\n",
    "\n",
    "为了建立、训练 graph 类型的模型，Python 程序首先建立一个计算图，然后调用 Session.run 来执行图。这提供了：\n",
    "\n",
    "    静态的自动微分\n",
    "    简单部署（在平台无关的服务器）\n",
    "    基于图的优化（常见的子表示式的合并，常量合并等）\n",
    "    编译和内核融合\n",
    "    自动分布式和复制（将节点放置在分布式系统上）\n",
    "\n",
    "为 eager 模式编写部署用的代码是非常困难的。难点1：从 model 中生成一个 graph；难点2：在服务器上直接运行Python控制台 及 代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 编写兼容的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager 模式的代码也可以用于 graph 模式。要实现这个转换，重开一个 Python 控制台，不开启 eager 即可。\n",
    "\n",
    "大多数的 TensorFlow op 兼容 eager 模式，但是请注意以下几点：\n",
    "\n",
    "    请使用 tf.data，而不是 queues。它更快且更易用。\n",
    "    请使用 “面向对象的 layer API” --------- 比如：tf.keras.layers，因为它们有显式的变量存储。\n",
    "    大多数的模型代码在 eager 和 graph 模式的行为一致，但也有例外（例如，动态模型使用 Python 控制流基于输入改变了模型的计算过程）。\n",
    "    Eager 模式的开始不可逆。想返回 graph 模式，请开启一个新的 Python 控制台。\n",
    "\n",
    "编写的代码最好同时支持 eager 和 graph 模式。这使得你可以在利用 eager 良好的交互、调试；同时拥有 graph 的分布式性能优势。\n",
    "\n",
    "在 eager 模式下进行写代码、调试、迭代，然后以 graph 来进行生产的部署。使用 tfe.Checkpoint 去保存、恢复模型变量这使得你可以很方便地在 eager 和 graph 模式之间切换。示例详见：tensorflow/contrib/eager/python/examples。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 在 graph 的局部使用 eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 eager 模式未开启的情况下，想在 graph 的局部使用 eager，可以使用 tfe.py_func。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def my_py_func(x):\n",
    "  x = tf.matmul(x, x)  # You can use tf ops\n",
    "  print(x)  # but it's eager!\n",
    "  return x\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  x = tf.placeholder(dtype=tf.float32)\n",
    "  # Call eager function in graph!\n",
    "  pf = tfe.py_func(my_py_func, [x], tf.float32)\n",
    "  sess.run(pf, feed_dict={x: [[2.0]]})  # [[4.0]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
